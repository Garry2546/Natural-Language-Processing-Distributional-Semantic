{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10035806,"sourceType":"datasetVersion","datasetId":6181458},{"sourceId":10056037,"sourceType":"datasetVersion","datasetId":6196463}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#IT SHOULD BE RUN ON KAGGLE\n\n\n!pip install tensorflow\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport random\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport os\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\n# For tokenization   # For stopword removal\n!pip install contractions\nimport spacy\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport re\nimport contractions\n\ndef set_seed(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n    os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.config.experimental.enable_op_determinism()  # Enforce deterministic ops in TensorFlow\n\nset_seed(42)  # Set a global seed\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T09:06:39.583463Z","iopub.execute_input":"2024-12-02T09:06:39.584783Z","iopub.status.idle":"2024-12-02T09:07:21.918432Z","shell.execute_reply.started":"2024-12-02T09:06:39.584722Z","shell.execute_reply":"2024-12-02T09:07:21.917319Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.64.1)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\nCollecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\nDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load the dataset\n#I have used relative path to load dataset, because in kaggle there is word count restriction while uploading the data, and in kaggle documentation they mentioned the method of keeping the training set in folder named dataset.\nfile_path = '../input/dataset/CW2-training-dataset.csv'  # Adjust the file path based on your Kaggle dataset\ndata = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\nprint(\"Dataset Loaded:\")\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:01:57.382337Z","iopub.execute_input":"2024-12-02T08:01:57.382638Z","iopub.status.idle":"2024-12-02T08:01:57.889439Z","shell.execute_reply.started":"2024-12-02T08:01:57.382610Z","shell.execute_reply":"2024-12-02T08:01:57.888632Z"}},"outputs":[{"name":"stdout","text":"Dataset Loaded:\n                                     ID                        title  \\\n0  8f5203de-b2f8-4c0c-b0c1-835ba92422e9                   Si wang ta   \n1  6416fe15-6f8a-41d4-8a78-3e8f120781c7          Shattered Vengeance   \n2  4979fe9a-0518-41cc-b85f-f364c91053ca                 L'esorciccio   \n3  b672850b-a1d9-44ed-9cff-025ee8b61e6f  Serendipity Through Seasons   \n4  b4d8e8cc-a53e-48f8-be6a-6432b928a56d                The Liability   \n\n                                       plot_synopsis  comedy  cult  flashback  \\\n0  After a recent amount of challenges, Billy Lo ...       0     0          0   \n1  In the crime-ridden city of Tremont, renowned ...       0     0          0   \n2  Lankester Merrin is a veteran Catholic priest ...       0     1          0   \n3  \"Serendipity Through Seasons\" is a heartwarmin...       0     0          0   \n4  Young and naive 19-year-old slacker, Adam (Jac...       0     0          1   \n\n   historical  murder  revenge  romantic  scifi  violence  \n0           0       1        1         0      0         1  \n1           0       1        1         1      0         1  \n2           0       0        0         0      0         0  \n3           0       0        0         1      0         0  \n4           0       0        0         0      0         0  \n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Identify genre columns (assuming the last 9 columns are the genres)\ngenre_columns = ['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']\n\n# Filter rows where all genre columns are 0\nrows_before = data.shape[0]\ndata_filtered = data[data[genre_columns].sum(axis=1) > 0]\ndata_filtered = data_filtered.sample(frac=1, random_state=42).reset_index(drop=True)\ndata_filtered['text'] = data_filtered['title'] + ' ' + data_filtered['plot_synopsis']\nrows_after = data_filtered.shape[0]\n\n# Calculate the number of rows removed\nremoved_movies_count = rows_before - rows_after\n\n# Display the result\nprint(f\"Number of movies removed (no genres): {removed_movies_count}\")\nprint(f\"Number of movies remaining: {rows_after}\")\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:01:57.890393Z","iopub.execute_input":"2024-12-02T08:01:57.890709Z","iopub.status.idle":"2024-12-02T08:01:57.955683Z","shell.execute_reply.started":"2024-12-02T08:01:57.890681Z","shell.execute_reply":"2024-12-02T08:01:57.954868Z"}},"outputs":[{"name":"stdout","text":"Number of movies removed (no genres): 0\nNumber of movies remaining: 8257\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Load spaCy's English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Define stopwords\nstop_words = set(stopwords.words('english'))\n\ndef normalize_contractions(text):\n    \"\"\"\n    Expands contractions in a given text.\n    Example: \"can't\" -> \"cannot\"\n    \"\"\"\n    return contractions.fix(text)\n\ndef preprocess_text(text):\n    text = normalize_contractions(text)\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphanumeric characters\n    doc = nlp(text)  # Process text with spaCy\n    # Tokenize, remove stopwords, and lemmatize\n    tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]\n    return ' '.join(tokens)  # Join lemmatized\n\n\n# Apply preprocessing to the 'plot_synopsis' column\ndata_filtered['cleaned_plot'] = data_filtered['plot_synopsis'].apply(preprocess_text)\n\n# Tokenize and pad sequences\nMAX_VOCAB_SIZE = 20000  # Define maximum vocabulary size\nMAX_SEQUENCE_LENGTH = 300  # Define maximum sequence length for padding\n\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(data_filtered['cleaned_plot'])\n\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(data_filtered['cleaned_plot'])\n\n# Pad sequences to ensure equal length\npadded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n\n# Vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\n\n# Display a sample of the processed data\nprint(\"Sample of preprocessed text with spaCy lemmatization and padded sequence:\")\nprint(data_filtered[['plot_synopsis', 'cleaned_plot']].head())\nprint(\"\\nSample Padded Sequence:\")\nprint(padded_sequences[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:01:57.957379Z","iopub.execute_input":"2024-12-02T08:01:57.957646Z","iopub.status.idle":"2024-12-02T08:14:45.788701Z","shell.execute_reply.started":"2024-12-02T08:01:57.957621Z","shell.execute_reply":"2024-12-02T08:14:45.787866Z"}},"outputs":[{"name":"stdout","text":"Sample of preprocessed text with spaCy lemmatization and padded sequence:\n                                       plot_synopsis  \\\n0  The film tells the story of Antonio Ricci, an ...   \n1  In the near future, the Moon has been colonize...   \n2  Two miners (Peter Hock & Harry Bellaver) find ...   \n3  Set in 12th-century medieval Japan, the film t...   \n4  Despite some deviations, the book's historical...   \n\n                                        cleaned_plot  \n0  film tell story antonio ricci unemployed man d...  \n1  near future moon colonize support station surf...  \n2  two miner peter hock   harry bellaver find yog...  \n3  set thcentury medieval japan film take place w...  \n4  despite deviation book historical framework ge...  \n\nSample Padded Sequence:\n[[   72     3   113 ...  3670   656     0]\n [  400   612  1312 ...     0     0     0]\n [   16  4147   276 ...   161   302  1965]\n [  109 12557  8362 ...     0     0     0]\n [  263     1   316 ...  1726  9509 16073]]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Load GloVe embeddings\ndef load_glove_embeddings(glove_file_path, embedding_dim):\n    embeddings_index = {}\n    with open(glove_file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    return embeddings_index\n\n# Path to GloVe embeddings (update with actual file path)\n# I have used glove.6B.300d.txt\nGLOVE_FILE_PATH = '../input/glove/glove.6B.300d.txt'\nEMBEDDING_DIM = 300  # Using GloVe with 300 dimensions\nglove_embeddings = load_glove_embeddings(GLOVE_FILE_PATH, EMBEDDING_DIM)\n\n# Create embedding matrix\nembedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in tokenizer.word_index.items():\n    if i >= vocab_size:\n        continue\n    embedding_vector = glove_embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Calculate class weights\ngenre_counts = data_filtered[genre_columns].sum()\ntotal_samples = len(data_filtered)\nclass_weights = {i: total_samples / (len(genre_columns) * count) for i, count in enumerate(genre_counts)}\n\n\nmodel = Sequential([\n    Embedding(input_dim=vocab_size, \n              output_dim=EMBEDDING_DIM, \n              input_length=MAX_SEQUENCE_LENGTH, \n              weights=[embedding_matrix], \n              trainable=False),\n    Bidirectional(LSTM(128, return_sequences=True)),\n    GlobalMaxPooling1D(),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(9, activation='sigmoid')\n])\n\n\nearly_stopping = EarlyStopping(\n    monitor='loss', patience=2, restore_best_weights=True\n)\n\n# Compile model\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0005),  # Adjust learning rate\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory = model.fit(\n    padded_sequences,\n    data_filtered[genre_columns].values,\n    epochs=20,\n    batch_size=64,  # Increased batch size for faster training\n    verbose=1,\n    class_weight=class_weights,\n    callbacks=[early_stopping]\n  # Handle class imbalance\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:14:45.789947Z","iopub.execute_input":"2024-12-02T08:14:45.790313Z","iopub.status.idle":"2024-12-02T08:16:56.700608Z","shell.execute_reply.started":"2024-12-02T08:14:45.790275Z","shell.execute_reply":"2024-12-02T08:16:56.699880Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - accuracy: 0.1599 - loss: 0.3197\nEpoch 2/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.1978 - loss: 0.2837\nEpoch 3/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.2405 - loss: 0.2683\nEpoch 4/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.2612 - loss: 0.2576\nEpoch 5/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.2580 - loss: 0.2530\nEpoch 6/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.2598 - loss: 0.2482\nEpoch 7/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.2709 - loss: 0.2430\nEpoch 8/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2774 - loss: 0.2377\nEpoch 9/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2917 - loss: 0.2319\nEpoch 10/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2865 - loss: 0.2304\nEpoch 11/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2904 - loss: 0.2233\nEpoch 12/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2927 - loss: 0.2199\nEpoch 13/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2999 - loss: 0.2165\nEpoch 14/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3123 - loss: 0.2144\nEpoch 15/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3221 - loss: 0.2096\nEpoch 16/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3163 - loss: 0.2069\nEpoch 17/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3242 - loss: 0.2037\nEpoch 18/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3349 - loss: 0.2019\nEpoch 19/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3411 - loss: 0.1978\nEpoch 20/20\n\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3292 - loss: 0.1986\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":" #Preprocess and Predict\ndef preprocess_and_predict(input_file_path, output_file_path, tokenizer, model, max_sequence_length, genre_columns, threshold=0.3):\n    \"\"\"\n    Preprocess the input dataset and make predictions using the trained model.\n    Dynamically handles datasets with varying structures (e.g., validation or test datasets).\n    \"\"\"\n    data = pd.read_csv(input_file_path)\n    text_column = 'plot_synopsis' if 'plot_synopsis' in data.columns else data.columns[-1]\n    id_column = 'ID' if 'ID' in data.columns else data.columns[0]\n\n    data['cleaned_plot'] = data[text_column].apply(preprocess_text)\n    sequences = tokenizer.texts_to_sequences(data['cleaned_plot'])\n    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n\n    predictions = model.predict(padded_sequences, batch_size=32)\n    binary_predictions = (predictions > threshold).astype(int)\n    for i in range(len(binary_predictions)):\n        if binary_predictions[i].sum() == 0:\n            top_genres = predictions[i].argsort()[-1:]\n            binary_predictions[i][top_genres] = 1\n\n    output_df = pd.DataFrame(binary_predictions, columns=genre_columns)\n    output_df.insert(0, 'ID', data[id_column])\n    output_df.to_csv(output_file_path, index=False)\n    print(f\"Predictions saved to: {output_file_path}\")\n\n# Example Usage for Test Data\ntest_file_path = '../input/dataset/CW2-test-dataset.csv'\noutput_test_path = '../working/11028972_task1_results.csv'\n\npreprocess_and_predict(\n    input_file_path=test_file_path,\n    output_file_path=output_test_path,\n    tokenizer=tokenizer,\n    model=model,\n    max_sequence_length=MAX_SEQUENCE_LENGTH,\n    genre_columns=genre_columns\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T08:16:56.702069Z","iopub.execute_input":"2024-12-02T08:16:56.702345Z","iopub.status.idle":"2024-12-02T08:18:50.240888Z","shell.execute_reply.started":"2024-12-02T08:16:56.702320Z","shell.execute_reply":"2024-12-02T08:18:50.239893Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\nPredictions saved to: ../working/11028972_task1_results.csv\n","output_type":"stream"}],"execution_count":32}]}