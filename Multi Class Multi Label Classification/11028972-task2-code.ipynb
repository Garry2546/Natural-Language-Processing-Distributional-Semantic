{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10035806,"sourceType":"datasetVersion","datasetId":6181458},{"sourceId":10056037,"sourceType":"datasetVersion","datasetId":6196463}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#IT SHOULD BE RUN IN KAGGLE\n\n!pip install transformers datasets\n!pip install transformers torch scikit-learn\n# Import libraries\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import precision_recall_fscore_support\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import RobertaTokenizer\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\nimport torch.nn as nn\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, AdamW, get_scheduler\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T10:31:47.797350Z","iopub.execute_input":"2024-12-02T10:31:47.797989Z","iopub.status.idle":"2024-12-02T10:32:05.782782Z","shell.execute_reply.started":"2024-12-02T10:31:47.797954Z","shell.execute_reply":"2024-12-02T10:32:05.781616Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Load dataset\nfile_path = '../input/dataset/CW2-training-dataset.csv'  # Adjust the path as needed\ndata = pd.read_csv(file_path)\n\nprint(\"Dataset Loaded:\")\nprint(data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T10:32:05.785099Z","iopub.execute_input":"2024-12-02T10:32:05.785517Z","iopub.status.idle":"2024-12-02T10:32:06.347474Z","shell.execute_reply.started":"2024-12-02T10:32:05.785476Z","shell.execute_reply":"2024-12-02T10:32:06.346464Z"}},"outputs":[{"name":"stdout","text":"Dataset Loaded:\n                                     ID                        title  \\\n0  8f5203de-b2f8-4c0c-b0c1-835ba92422e9                   Si wang ta   \n1  6416fe15-6f8a-41d4-8a78-3e8f120781c7          Shattered Vengeance   \n2  4979fe9a-0518-41cc-b85f-f364c91053ca                 L'esorciccio   \n3  b672850b-a1d9-44ed-9cff-025ee8b61e6f  Serendipity Through Seasons   \n4  b4d8e8cc-a53e-48f8-be6a-6432b928a56d                The Liability   \n\n                                       plot_synopsis  comedy  cult  flashback  \\\n0  After a recent amount of challenges, Billy Lo ...       0     0          0   \n1  In the crime-ridden city of Tremont, renowned ...       0     0          0   \n2  Lankester Merrin is a veteran Catholic priest ...       0     1          0   \n3  \"Serendipity Through Seasons\" is a heartwarmin...       0     0          0   \n4  Young and naive 19-year-old slacker, Adam (Jac...       0     0          1   \n\n   historical  murder  revenge  romantic  scifi  violence  \n0           0       1        1         0      0         1  \n1           0       1        1         1      0         1  \n2           0       0        0         0      0         0  \n3           0       0        0         1      0         0  \n4           0       0        0         0      0         0  \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\n\n# Initialize the tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n# Tokenize plot_synopsis\ndef tokenize_synopses(data, tokenizer, max_length=512):\n    \"\"\"\n    Tokenizes the plot_synopsis column or the last column dynamically \n    if plot_synopsis does not exist.\n    \"\"\"\n    # Dynamically detect the text column\n    text_column = 'plot_synopsis' if 'plot_synopsis' in data.columns else data.columns[-1]\n    \n    tokenized = tokenizer(\n        data[text_column].tolist(),\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n    return tokenized\n\n# Apply tokenization\nmax_seq_length = 256  # Adjust as needed\ntokenized_data = tokenize_synopses(data, tokenizer, max_length=max_seq_length)\n\n# Check tokenized output\nprint(\"Tokenized Data Shapes:\", tokenized_data['input_ids'].shape, tokenized_data['attention_mask'].shape)\n\n# Check if the dataset is training/validation or test\nif all(col in data.columns for col in [\"comedy\", \"cult\", \"flashback\", \"historical\", \"murder\", \"revenge\", \"romantic\", \"scifi\", \"violence\"]):\n    # Training or Validation dataset\n    labels = torch.tensor(data[[\"comedy\", \"cult\", \"flashback\", \"historical\", \"murder\", \"revenge\", \"romantic\", \"scifi\", \"violence\"]].values, dtype=torch.float32)\n    print(\"Labels shape:\", labels.shape)\n\n    # Compute class weights for handling imbalance\n    def compute_class_weights(data):\n        \"\"\"\n        Computes class weights for multi-label data.\n        \"\"\"\n        labels = data[[\"comedy\", \"cult\", \"flashback\", \"historical\", \"murder\", \"revenge\", \"romantic\", \"scifi\", \"violence\"]].values\n        weights = []\n        for i in range(labels.shape[1]):\n            class_weight = compute_class_weight(\n                class_weight='balanced',\n                classes=[0, 1],\n                y=labels[:, i]\n            )\n            weights.append(class_weight[1])  # Positive class weight\n        return torch.tensor(weights, dtype=torch.float32)\n\n    # Compute weights\n    class_weights = compute_class_weights(data)\n    print(\"Class Weights:\", class_weights)\n\n    # Create TensorDataset for PyTorch DataLoader\n    dataset = TensorDataset(\n        tokenized_data['input_ids'], \n        tokenized_data['attention_mask'], \n        labels\n    )\n\n    # Define DataLoader\n    batch_size = 16  # Adjust based on memory availability\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    # Check DataLoader\n    for batch in dataloader:\n        input_ids, attention_mask, batch_labels = batch\n        print(\"Input IDs shape:\", input_ids.shape)\n        print(\"Attention Mask shape:\", attention_mask.shape)\n        print(\"Labels shape:\", batch_labels.shape)\n        break\n\nelse:\n    # Test dataset (no labels)\n    labels = None\n    print(\"No labels found. Skipping DataLoader creation for test dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T10:32:06.349374Z","iopub.execute_input":"2024-12-02T10:32:06.349828Z","iopub.status.idle":"2024-12-02T10:32:52.358746Z","shell.execute_reply.started":"2024-12-02T10:32:06.349780Z","shell.execute_reply":"2024-12-02T10:32:52.357671Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Tokenized Data Shapes: torch.Size([8257, 256]) torch.Size([8257, 256])\nLabels shape: torch.Size([8257, 9])\nClass Weights: tensor([ 3.2714,  2.2923,  2.0705, 22.1962,  1.0272,  2.4916,  2.0581, 20.2377,\n         1.3474])\nInput IDs shape: torch.Size([16, 256])\nAttention Mask shape: torch.Size([16, 256])\nLabels shape: torch.Size([16, 9])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Determine the number of labels dynamically\nnum_labels = batch_labels.shape[1]  # This will ensure num_labels matches the dataset\n\n# Define advanced model with dropout\nclass AdvancedRobertaClassifier(nn.Module):\n    def __init__(self, model_name=\"roberta-base\", num_labels=num_labels, dropout_rate=0.3):\n        super(AdvancedRobertaClassifier, self).__init__()\n        self.roberta = RobertaForSequenceClassification.from_pretrained(\n            model_name, num_labels=num_labels, problem_type=\"multi_label_classification\"\n        )\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.dropout(outputs.logits)\n        return logits\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize model with the correct number of labels\nmodel = AdvancedRobertaClassifier(model_name=\"roberta-base\", num_labels=num_labels, dropout_rate=0.3)\nmodel.to(device)\n\n# Define optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\n\n\n# Define optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nepochs = 5\nnum_training_steps = len(dataloader) * epochs\nscheduler = get_scheduler(\n    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Define loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Training loop with mixed precision and gradient accumulation\ndef train_model(model, dataloader, optimizer, scheduler, criterion, epochs=5, accumulation_steps=4):\n    \"\"\"\n    Trains the model using mixed precision and gradient accumulation.\n    \"\"\"\n    scaler = GradScaler()\n    model.train()\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        total_loss = 0\n        optimizer.zero_grad()\n\n        for i, batch in enumerate(tqdm(dataloader)):\n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            # Mixed precision forward pass\n            with autocast():\n                logits = model(input_ids, attention_mask)\n                loss = criterion(logits, labels)\n                total_loss += loss.item()\n\n            # Backward pass\n            scaler.scale(loss).backward()\n\n            # Gradient accumulation\n            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n\n        avg_loss = total_loss / len(dataloader)\n        print(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n\n# Train the model\ntrain_model(model, dataloader, optimizer, scheduler, criterion, epochs=5, accumulation_steps=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T10:32:52.360373Z","iopub.execute_input":"2024-12-02T10:32:52.361081Z","iopub.status.idle":"2024-12-02T10:42:29.786315Z","shell.execute_reply.started":"2024-12-02T10:32:52.361022Z","shell.execute_reply":"2024-12-02T10:42:29.785387Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/tmp/ipykernel_30/1310598868.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/517 [00:00<?, ?it/s]/tmp/ipykernel_30/1310598868.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n100%|██████████| 517/517 [01:56<00:00,  4.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Average Loss: 0.5302232068673324\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 517/517 [01:54<00:00,  4.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Average Loss: 0.48521665995310076\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 517/517 [01:55<00:00,  4.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Average Loss: 0.4704490538607252\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 517/517 [01:55<00:00,  4.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Average Loss: 0.4546010616438993\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 517/517 [01:55<00:00,  4.49it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Average Loss: 0.44046224287676855\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def count_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total_params, trainable_params\n\n# Count and print parameters\ntotal_params, trainable_params = count_parameters(model)\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Trainable parameters: {trainable_params}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T10:42:29.789579Z","iopub.execute_input":"2024-12-02T10:42:29.790303Z","iopub.status.idle":"2024-12-02T10:42:29.797597Z","shell.execute_reply.started":"2024-12-02T10:42:29.790271Z","shell.execute_reply":"2024-12-02T10:42:29.796668Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 124652553\nTrainable parameters: 124652553\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"validation_file_path = \"../input/dataset/CW2-test-dataset.csv\"  # Update path if needed\ndata = pd.read_csv(validation_file_path)\n\n# Check if it's validation or test dataset\nis_validation = all(col in data.columns for col in [\"comedy\", \"cult\", \"flashback\", \"historical\", \"murder\", \"revenge\", \"romantic\", \"scifi\", \"violence\"])\n\nif is_validation:\n    print(\"Validation dataset detected.\")\nelse:\n    print(\"Test dataset detected.\")\n\n# Tokenize the plot_synopsis column\ntext_column = 'plot_synopsis' if 'plot_synopsis' in data.columns else data.columns[-1]\ntokenized_data = tokenize_synopses(data, tokenizer, max_length=max_seq_length)\n\n# Move tokenized inputs to GPU\ninput_ids_val = tokenized_data['input_ids'].to(device)\nattention_mask_val = tokenized_data['attention_mask'].to(device)\n\n# Predict on the dataset and save results\nmodel.eval()\nresults = []\ntrue_labels = []\n\nwith torch.no_grad():\n    for i in tqdm(range(len(input_ids_val))):\n        input_ids = input_ids_val[i].unsqueeze(0)\n        attention_mask = attention_mask_val[i].unsqueeze(0)\n\n        # Forward pass\n        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert logits to binary predictions (threshold = 0.5)\n        predictions = (logits > 0.5).astype(int).flatten().tolist()\n\n        # Append results with movie ID and predictions\n        movie_id = data.iloc[i, 0]  # Assuming the first column in the dataset is \"ID\"\n        results.append([movie_id] + predictions)\n\n        # If validation dataset, collect true labels for evaluation\n        if is_validation:\n            true_labels.append(data.iloc[i, 1:].values.tolist())  # Assuming genre columns are after the ID column\n\n# Save predictions to CSV\ncolumns = [\"ID\", \"comedy\", \"cult\", \"flashback\", \"historical\", \"murder\", \"revenge\", \"romantic\", \"scifi\", \"violence\"]\nresult_df = pd.DataFrame(results, columns=columns)\nresult_df.to_csv(\"../working/11028972_task2_results.csv\", index=False)\nprint(\"Predictions saved to 11028972_task2_results.csv\")\n\n# If validation dataset, compute metrics\nif is_validation:\n    from sklearn.metrics import precision_recall_fscore_support\n\n    # Ensure true labels are numeric\n    true_labels = data[[\"comedy\", \"cult\", \"flashback\", \"historical\", \"murder\", \"revenge\", \"romantic\", \"scifi\", \"violence\"]].values\n    true_labels = true_labels.astype(float)  # Convert to float if necessary\n\n    # Convert predictions to numpy format, excluding the ID column\n    y_pred = np.array([row[1:] for row in results], dtype=float)\n\n    # Compute metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, y_pred, average='weighted')\n    print(f\"Validation Metrics - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T10:42:29.799135Z","iopub.execute_input":"2024-12-02T10:42:29.799797Z","iopub.status.idle":"2024-12-02T10:43:00.439939Z","shell.execute_reply.started":"2024-12-02T10:42:29.799739Z","shell.execute_reply":"2024-12-02T10:43:00.438999Z"}},"outputs":[{"name":"stdout","text":"Test dataset detected.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1204/1204 [00:23<00:00, 50.49it/s]","output_type":"stream"},{"name":"stdout","text":"Predictions saved to 11028972_task2_results.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17}]}