{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOd8tfnCr8+VWu+kj8QuE6C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUQd_r2bQk4H","executionInfo":{"status":"ok","timestamp":1729869434706,"user_tz":-60,"elapsed":5697,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"da2497c0-eeed-40f0-e58e-a6a4debc3bf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"]}],"source":["pip install nltk gensim scikit-learn numpy"]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","\n","import string\n","import re\n","import pandas as pd\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from gensim.models import Phrases\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from nltk.corpus.reader import PlaintextCorpusReader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smyez4wBR8oD","executionInfo":{"status":"ok","timestamp":1729869435079,"user_tz":-60,"elapsed":377,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"53f60d3f-e781-4239-c5ba-699e61b76222"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["with open('./data/WikiText-103.txt', 'r', encoding='utf-8') as file:\n","    corpus = file.read()\n","# Define a regex pattern to match headings (sections surrounded by \"=\" signs)\n","my_heading = r'(\\n (= )+[^=]*[^=](= )+\\n )'\n","\n","# Split the raw text based on headings and get documents\n","doc_split = re.split(my_heading, corpus)\n","\n","# Extract the articles/documents (ignoring the headings)\n","docs = [x.strip(\"\\n \") for x in doc_split[2::2] if x != \"= \"]\n","\n","# Remove empty articles/documents\n","documents = [doc.strip() for doc in docs if doc.strip() != '']\n","\n","print(f\"Number of documents: {len(documents)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0Y3SxcJSIjm","executionInfo":{"status":"ok","timestamp":1729869439827,"user_tz":-60,"elapsed":4753,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"011f0080-cbc7-4cf5-dce2-5f1099149aff"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of documents: 89698\n"]}]},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# Preprocessing function\n","def preprocess_document(text):\n","    # Tokenize the document into lowercase words\n","    tokens = word_tokenize(text.lower())\n","\n","    # Remove non-alphabetic tokens and stopwords\n","    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n","\n","    # Lemmatize the remaining tokens (default to noun)\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n","\n","    return lemmatized_tokens\n","\n","# Assuming 'documents' is your input corpus (list of raw text documents)\n","# Preprocess each document in the 'documents' list\n","preprocessed_documents = [preprocess_document(doc) for doc in documents]\n"],"metadata":{"id":"lRTMjB6PUoiA","executionInfo":{"status":"ok","timestamp":1729869814332,"user_tz":-60,"elapsed":374508,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["bigram = Phrases(preprocessed_documents, min_count=5, threshold=10)\n","trigram = Phrases(bigram[preprocessed_documents], threshold=10)\n","\n","def make_bigrams(texts):\n","    return [bigram[doc] for doc in texts]\n","\n","def make_trigrams(texts):\n","    return [trigram[bigram[doc]] for doc in texts]\n","\n","# Apply bigrams and trigrams\n","corpus_bigrams = make_bigrams(preprocessed_documents)\n","corpus_trigrams = make_trigrams(corpus_bigrams)"],"metadata":{"id":"p4DEdphcBJgb","executionInfo":{"status":"ok","timestamp":1729870051073,"user_tz":-60,"elapsed":236767,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# Step 1: Preprocess the Corpus (assuming corpus_preprocessed is already prepared)\n","corpus_tokens = corpus_trigrams\n","\n","# Step 2: Train Word2Vec (Skip-gram) Model\n","word2vec_model = Word2Vec(sentences=corpus_tokens, vector_size=300, window=8, sg=0, min_count=1, workers=4)\n","vocab_size = len(word2vec_model.wv)\n","print(f\"Vocabulary size: {vocab_size}\")\n","\n","# Step 3: Read the Test CSV File (with 'Gold' column, which we will later drop)\n","test_file_path = './data/CW-1-testdata.csv'  # Path to your test pairs CSV file\n","test_pairs = pd.read_csv(test_file_path, header=None, names=['PairID', 'Term1', 'Term2', 'GoldSimilarity'])\n","\n","# Step 4: Create a function to calculate cosine similarity between two terms using Word2Vec\n","def calculate_word2vec_similarity(word1, word2, model):\n","    try:\n","        if word1 in model.wv and word2 in model.wv:\n","            vec1 = model.wv[word1]\n","            vec2 = model.wv[word2]\n","            similarity = cosine_similarity([vec1], [vec2])[0][0]\n","            return similarity\n","        else:\n","            return 0\n","    except KeyError:\n","        return 0\n","\n","# Step 5: Use a For Loop to calculate the cosine similarity and add it to a new column\n","computed_similarities = []\n","for idx, row in test_pairs.iterrows():\n","    word1 = row['Term1']\n","    word2 = row['Term2']\n","\n","    # Calculate the cosine similarity using the custom function\n","    similarity = calculate_word2vec_similarity(word1, word2, word2vec_model)\n","\n","    # Append the similarity to the list\n","    computed_similarities.append(similarity)\n","\n","# Step 6: Add the computed similarities to the DataFrame as a new column\n","test_pairs['computed_similarity'] = computed_similarities\n","\n","# Step 7: Drop the 'GoldSimilarity' column and keep only relevant columns (PairID, Term1, Term2, computed_similarity)\n","output_df = test_pairs[['PairID', 'Term1', 'Term2', 'computed_similarity']]\n","\n","# Step 8: Write the updated results to a new CSV file\n","output_file_path = './data/11028972_task2_results.csv'\n","output_df.to_csv(output_file_path, index=False, header=False)\n","\n","print(f\"Final results written to {output_file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qA_C96ZYU0r2","executionInfo":{"status":"ok","timestamp":1729874864026,"user_tz":-60,"elapsed":321641,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"e2bf4010-82af-4c6b-d807-866bb21bce48"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 280087\n","Final results written to ./data/11028972_task2_results.csv\n"]}]}]}