{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPA3TEaQJKPo0ZmTtchVfbO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":70,"metadata":{"id":"nndvo9hrYWt8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729869484581,"user_tz":-60,"elapsed":5477,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"7241a0e4-b05e-44c9-9c8c-98d66d11638c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"]}],"source":["pip install nltk gensim scikit-learn numpy\n"]},{"cell_type":"code","source":["!pip install --upgrade gensim\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuXiHsU5zz77","executionInfo":{"status":"ok","timestamp":1729869489244,"user_tz":-60,"elapsed":4668,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"82d8116a-c2e0-430d-f4e7-8f46e782db33"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","import string\n","import re\n","import pandas as pd\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from gensim.models import Phrases\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from gensim.models import Phrases\n","\n","\n","from nltk.corpus.reader import PlaintextCorpusReader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZCnp0Q8ipLf","executionInfo":{"status":"ok","timestamp":1729869489630,"user_tz":-60,"elapsed":388,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"c504ffe8-de53-40e7-d57e-f0530d3de70b"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["with open('./data/WikiText-103.txt', 'r', encoding='utf-8') as file:\n","    corpus = file.read()\n","# Define a regex pattern to match headings (sections surrounded by \"=\" signs)\n","my_heading = r'(\\n (= )+[^=]*[^=](= )+\\n )'\n","\n","# Split the raw text based on headings and get documents\n","doc_split = re.split(my_heading, corpus)\n","\n","# Extract the articles/documents (ignoring the headings)\n","docs = [x.strip(\"\\n \") for x in doc_split[2::2] if x != \"= \"]\n","\n","# Remove empty articles/documents\n","documents = [doc.strip() for doc in docs if doc.strip() != '']\n","\n","print(f\"Number of documents: {len(documents)}\")"],"metadata":{"id":"KJ3KRN2MgbED","executionInfo":{"status":"ok","timestamp":1729869492746,"user_tz":-60,"elapsed":3119,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3349c94b-4d16-4ee9-9a3f-601c5ab46180"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of documents: 89698\n"]}]},{"cell_type":"code","source":["# Initialize stopwords and lemmatizer\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# Function to preprocess each document\n","def preprocess_document(text):\n","    # Tokenization\n","    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n","\n","    # Remove non-alphabetic tokens and stopwords\n","    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n","\n","    # Lemmatization (without POS tagging, defaults to noun)\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n","\n","    return lemmatized_tokens\n","\n","# Preprocess each document in the 'documents' list\n","preprocessed_documents = [preprocess_document(doc) for doc in documents]\n","\n","# Example of preprocessing output for the first document\n","print(f\"First preprocessed document: {preprocessed_documents[0]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzZEMKxY1SUc","executionInfo":{"status":"ok","timestamp":1729869862902,"user_tz":-60,"elapsed":370158,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"outputId":"d7a4dcad-cf81-4b18-b403-96aa9f59f87b"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["First preprocessed document: ['previous', 'unk', 'chronicle', 'game', 'valkyria', 'chronicle', 'iii', 'tactical', 'role', 'playing', 'game', 'player', 'take', 'control', 'military', 'unit', 'take', 'part', 'mission', 'enemy', 'force', 'story', 'told', 'comic', 'book', 'like', 'panel', 'animated', 'character', 'portrait', 'character', 'speaking', 'partially', 'voiced', 'speech', 'bubble', 'partially', 'unvoiced', 'text', 'player', 'progress', 'series', 'linear', 'mission', 'gradually', 'unlocked', 'map', 'freely', 'scanned', 'replayed', 'unlocked', 'route', 'story', 'location', 'map', 'varies', 'depending', 'individual', 'player', 'approach', 'one', 'option', 'selected', 'sealed', 'player', 'outside', 'mission', 'player', 'character', 'rest', 'camp', 'unit', 'customized', 'character', 'growth', 'occurs', 'alongside', 'main', 'story', 'mission', 'character', 'specific', 'sub', 'mission', 'relating', 'different', 'squad', 'member', 'game', 'completion', 'additional', 'episode', 'unlocked', 'higher', 'difficulty', 'found', 'rest', 'game', 'also', 'love', 'simulation', 'element', 'related', 'game', 'two', 'main', 'heroine', 'although', 'take', 'minor', 'role', 'game', 'battle', 'system', 'unk', 'system', 'carried', 'directly', 'unk', 'chronicle', 'mission', 'player', 'select', 'unit', 'using', 'top', 'perspective', 'battlefield', 'map', 'character', 'selected', 'player', 'move', 'character', 'around', 'battlefield', 'third', 'person', 'character', 'act', 'per', 'turn', 'character', 'granted', 'multiple', 'turn', 'expense', 'character', 'turn', 'character', 'field', 'distance', 'movement', 'limited', 'action', 'gauge', 'nine', 'character', 'assigned', 'single', 'mission', 'gameplay', 'character', 'call', 'something', 'happens', 'health', 'point', 'hp', 'getting', 'low', 'knocked', 'enemy', 'attack', 'character', 'specific', 'potential', 'skill', 'unique', 'character', 'divided', 'personal', 'potential', 'innate', 'skill', 'remain', 'unaltered', 'unless', 'otherwise', 'dictated', 'story', 'either', 'help', 'impede', 'character', 'battle', 'potential', 'grown', 'throughout', 'game', 'always', 'grant', 'boon', 'character', 'learn', 'battle', 'potential', 'character', 'unique', 'master', 'table', 'grid', 'based', 'skill', 'table', 'used', 'acquire', 'link', 'different', 'skill', 'character', 'also', 'special', 'ability', 'grant', 'temporary', 'boost', 'battlefield', 'kurt', 'activate', 'direct', 'command', 'move', 'around', 'battlefield', 'without', 'depleting', 'action', 'point', 'gauge', 'character', 'unk', 'shift', 'valkyria', 'form', 'become', 'invincible', 'imca', 'target', 'multiple', 'enemy', 'unit', 'heavy', 'weapon', 'troop', 'divided', 'five', 'class', 'scout', 'unk', 'engineer', 'lancer', 'armored', 'soldier', 'trooper', 'switch', 'class', 'changing', 'assigned', 'weapon', 'changing', 'class', 'greatly', 'affect', 'stats', 'gained', 'previous', 'class', 'victory', 'battle', 'experience', 'point', 'awarded', 'squad', 'distributed', 'five', 'different', 'attribute', 'shared', 'entire', 'squad', 'feature', 'differing', 'early', 'game', 'method', 'distributing', 'different', 'unit', 'type']\n"]}]},{"cell_type":"code","source":["# Train Bigram and Trigram Models to Detect Multi-word Terms\n","bigram = Phrases(preprocessed_documents, min_count=7, threshold=10)\n","trigram = Phrases(bigram[preprocessed_documents], threshold=10)\n","\n","# Apply bigram and trigram models to detect multi-word terms\n","def make_bigrams(texts):\n","    return [bigram[doc] for doc in texts]  # Using bigram directly\n","\n","def make_trigrams(texts):\n","    return [trigram[bigram[doc]] for doc in texts]  # Using trigram directly\n","\n","# Apply bigrams and trigrams\n","corpus_bigrams = make_bigrams(preprocessed_documents)\n","corpus_trigrams = make_trigrams(corpus_bigrams)\n","\n","# Convert Preprocessed Documents (Trigrams) Back to String Format\n","corpus_preprocessed = [' '.join(doc) for doc in corpus_trigrams]\n"],"metadata":{"id":"vcJ9bhNNzKlz","executionInfo":{"status":"ok","timestamp":1729870111317,"user_tz":-60,"elapsed":248418,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# Step 1: Create TF-IDF Vectorizer and Fit on the Corpus\n","vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, smooth_idf=True, sublinear_tf=True)\n","tfidf_matrix = vectorizer.fit_transform(corpus_preprocessed)\n","vocab_size = len(vectorizer.vocabulary_)\n","print(f\"Vocabulary size: {vocab_size}\")\n","\n","# Step 2: Read the Test Data\n","# Assuming we have a test dataset in this format (replace with your actual data)\n","\n","# Step 3: Read the Test CSV File and Remove the Fourth Column\n","test_file_path = './data/CW-1-testdata.csv'  # Path to your test pairs CSV file\n","test_pairs = pd.read_csv(test_file_path, header=None, names=['PairID', 'Term1', 'Term2', 'GoldSimilarity'])\n","\n","# Replace spaces with underscores in multi-word terms (if needed)\n","test_pairs['Term1'] = test_pairs['Term1'].str.replace(' ', '_')\n","test_pairs['Term2'] = test_pairs['Term2'].str.replace(' ', '_')\n","\n","# Step 4: Create a function to calculate cosine similarity for word pairs\n","def calculate_cosine_similarity(row):\n","    word1_vec = get_term_vector(row['Term1'], vectorizer, tfidf_matrix)\n","    word2_vec = get_term_vector(row['Term2'], vectorizer, tfidf_matrix)\n","\n","    if word1_vec is not None and word2_vec is not None:\n","        return cosine_similarity([word1_vec], [word2_vec])[0][0]\n","    return 0  # Return 0 if either word is not found in the vocabulary\n","\n","# Function to get term vector from TF-IDF matrix\n","def get_term_vector(term, vectorizer, tfidf_matrix):\n","    try:\n","        term_index = vectorizer.vocabulary_[term]\n","        return tfidf_matrix[:, term_index].toarray().flatten()\n","    except KeyError:\n","        return None  # Return None if the term is not in the vocabulary\n","\n","# Step 5: Apply the Cosine Similarity Calculation to Each Row in Test Pairs\n","for idx, row in test_pairs.iterrows():\n","    word1, word2 = row['Term1'], row['Term2']\n","\n","    # Get term vectors for the pair\n","    vec1 = get_term_vector(word1, vectorizer, tfidf_matrix)\n","    vec2 = get_term_vector(word2, vectorizer, tfidf_matrix)\n","\n","    # Calculate cosine similarity if both vectors are found\n","    if vec1 is not None and vec2 is not None:\n","        similarity = cosine_similarity([vec1], [vec2])[0][0]\n","    else:\n","        similarity = 0  # Assign 0 similarity if any term is not in vocabulary\n","    test_pairs.at[idx, 'computed_similarity'] = similarity\n","\n","# Step 6: Write the Updated Test Pairs with Calculated Cosine Similarities to the CSV File\n","output_df = test_pairs[['PairID', 'Term1', 'Term2', 'computed_similarity']]  # Note the correct column name here\n","output_file_path = './data/11028972_task1_results.csv'  # Path to save the updated CSV\n","output_df.to_csv(output_file_path, index=False, header=False)\n","\n","print(f\"Updated test pairs written to {output_file_path}\")"],"metadata":{"id":"soWi4szk69jv","executionInfo":{"status":"ok","timestamp":1729874412329,"user_tz":-60,"elapsed":41684,"user":{"displayName":"Geetinder Singh","userId":"15452010732766666190"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4cd8629b-c311-401f-b7eb-0101200f72f9"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 272445\n","Updated test pairs written to ./data/11028972_task1_results.csv\n"]}]}]}